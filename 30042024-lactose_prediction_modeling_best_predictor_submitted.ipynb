{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c26e514-03b8-4c05-9488-1223b84a7962",
   "metadata": {},
   "source": [
    "## MLGIG Team â€“  Milk Lactose Prediction Data Challenge\n",
    "4th International Workshop on Spectroscopy and Chemometrics 2024, organised by the VistaMilk SFI Research Centre\n",
    "\n",
    "Data description:\n",
    "\n",
    "A total of 72 milk samples were analysed using hyperspectral imaging. The number of pixels considered for each sample ranges from 112 to 300. For each pixel, a spectrum containing 3,424 wavelengths in the range from 400 cm-1 to 7,000 cm-1 was generated. For each sample, the spectra were extracted from the pixels and included into a single dataset, therefore the spatial information were successively lost.  \n",
    "** Target: For each sample the lactose concentration was analysed and expressed as mg/mL. **\n",
    "\n",
    "Data provided:\n",
    "\n",
    "Training set covariates: an excel file with 64 different sheets, each one corresponding to a different sample. Each sheet corresponds to a data matrix with ni rows and p = 3424 columns, corresponding to the spectra for the ni considered pixels for the i-th sample. \n",
    "Training set response: an excel file containing the information on the lactose content for the samples included in the training set. \n",
    "Test set covariates: an excel file with 8 different sheets, with the same structure as the training set covariates but without the information on the lactose content. \n",
    "\n",
    "Task:\n",
    "\n",
    "Participants should develop prediction models to quantify the lactose content employing the spectral information. Each participant should send a csv file with the predicted lactose contents for the samples in the test set. \n",
    "\n",
    "Tips:\n",
    "\n",
    "For each sample there are both outlier spectra and noise regions to be deleted before the development of the prediction model. \n",
    "\n",
    "Details about event:\n",
    "https://www.eventbrite.com/e/international-workshop-on-spectroscopy-chemometrics-tickets-844689056707?keep_tld=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb41fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T12:19:21.891534Z",
     "start_time": "2023-11-03T12:19:21.847712Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from numpy import zeros, newaxis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import seaborn as sns\n",
    "#sns.set()\n",
    "import os, sys\n",
    "import time\n",
    "import timeit\n",
    "#linear models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "#ensembles\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "#knn\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#neural networks\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#svm: try both linear kernel and rbf kernel\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#deep neural networks aka deep learning\n",
    "#tbd: initial results by Ashish did not look good\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score \n",
    "#from patsy import dmatrices\n",
    "from sklearn.utils import shuffle\n",
    "#from pandas_profiling import ProfileReport\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "#feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Import package matplotlib for visualisation/plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#For showing plots directly in the notebook run the command below\n",
    "%matplotlib inline\n",
    "# For saving multiple plots into a single pdf file\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "from aeon.transformations.collection.convolution_based import Rocket, MiniRocket, HydraTransformer,  MultiRocket, MiniRocketMultivariate, MultiRocketMultivariate\n",
    "from aeon.transformations.collection.interval_based import QUANTTransformer, RandomIntervals\n",
    "from aeon.transformations.collection.shapelet_based import RandomDilatedShapeletTransform, RandomShapeletTransform\n",
    "\n",
    "from aeon.regression.deep_learning import CNNRegressor, FCNRegressor, ResNetRegressor, IndividualInceptionRegressor, MLPRegressor\n",
    "from aeon.regression.interval_based import TimeSeriesForestRegressor, DrCIFRegressor\t\n",
    "from aeon.regression.distance_based import KNeighborsTimeSeriesRegressor\n",
    "from aeon.regression.shapelet_based import RDSTRegressor\n",
    "from aeon.regression.feature_based import FreshPRINCERegressor\t\n",
    "from aeon.regression.hybrid import RISTRegressor\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "#Quant, Hydra, Weasel2, MrSQM\n",
    "\n",
    "from aeon.transformations.difference import Differencer\n",
    "\n",
    "#package for ordinal regression\n",
    "import mord\n",
    "\n",
    "from aeon.classification.ordinal_classification import OrdinalTDE\n",
    "#from aeon.classification.interval_based import QUANTClassifier\n",
    "#from aeon.classification.convolution_based import HydraClassifier\n",
    "#from aeon.classification.dictionary_based import MUSE\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIG_SIZE = 12\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mrsqm\n",
    "\n",
    "plt.rc('font', size=BIG_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIG_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIG_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=BIG_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=BIG_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=BIG_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIG_SIZE)  # fontsize of the figure title\n",
    "\n",
    "np.set_printoptions(precision=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413079ac-63df-4dd7-bf6e-f9dddae12fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.aeon-toolkit.org/en/stable/examples/classification/classification.html\n",
    "# From aeon: We recommend storing time series in 3D numpy array of shape (n_instances, n_channels, n_timepoints) \n",
    "# and where possible our single problem loaders will return a 3D numpy.\n",
    "\n",
    "# load each sample in a 3d numpy array (n_samples, n_channels, n_timepoints)\n",
    "train_X = np.load(\"train_X.npy\")\n",
    "test_X = np.load(\"test_X.npy\")\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_X[0])\n",
    "\n",
    "print(test_X.shape)\n",
    "print(test_X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7c83b-78ac-4a83-8683-72d8f5902272",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = pd.read_excel('train_targets.xlsx', 0, engine='openpyxl')\n",
    "print(train_targets.shape)\n",
    "print(train_targets)\n",
    "\n",
    "train_y = train_targets['lactose content']\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9f654-a2d2-4848-8942-fd012cc9d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8872f-a55e-427c-8eea-acb98ef75f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.hist(bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfd9df-486c-4114-9b8a-b82c622e8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.sort_values().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121c1e9-4561-443b-91ce-9ef23dddbbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWaves(df, features, target, out_file=\"plot_sample.png\"):\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    plt.plot(df[features].T, color='LightBlue')\n",
    "    plt.plot(df[features].mean().T, color='Red')\n",
    "    \n",
    "    #plt.xticks(rotation=45, ha='right')\n",
    "    #plt.axes().xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "    #plt.axes().xaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "    #plt.vlines(540, ymin=0, ymax=4.5, color='black', linestyle='--')\n",
    "    plt.title('Matrix sample and overall column-wise mean')\n",
    "    plt.legend(target)\n",
    "    #plt.grid()\n",
    "    plt.savefig(out_file)\n",
    "    \n",
    "#if __name__ == \"__main__\":\n",
    "#    plotWaves(df = df, features = df.columns, target=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277fe09-a15b-4487-b483-b28e6a0d9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(3):\n",
    "    print(\"sample:\", sample)\n",
    "    df = pd.DataFrame(train_X[sample])\n",
    "    #print(df)\n",
    "    #print(df.describe().T)\n",
    "    #print(train_y[sample])\n",
    "    plotWaves(df = df, features = df.columns, target = [str(train_y[sample])], out_file=\"plot_train_sample\" + str(sample) + \"-target\" + str(train_y[sample]))\n",
    "    #plotWaves(df = df, features = df.columns, target = [\"NA\"], out_file=\"plot_test_sample\" + str(sample) + \"-target-na\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2a335-f68d-4967-8c5e-0316982bb736",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [train_y.mean()] * len(train_y)\n",
    "print(f'Baseline RMSE: {math.sqrt(mean_squared_error(train_y, y_pred))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35aaf6-6ff7-4245-a8cf-6cba255d5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_split_experiment(X, y, model, test_split = 0.25, random_state = 42, stratify = None):\n",
    "    #rng = np.random.default_rng(random_state)    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split, random_state=random_state, stratify=stratify, shuffle=True)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    #print(rmse, r2, y_pred, y_test)\n",
    "    return rmse, r2, y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4028f7-9c6a-46ba-a5df-037ede2e31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_experiment(X, y, model, cv=4, random_state = 42, features = None):\n",
    "\n",
    "    #rng = np.random.default_rng(random_state)  \n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state= random_state)\n",
    "    scores = cross_validate(model, X, y, cv=skf, scoring=('r2', 'neg_mean_squared_error'))\n",
    "    \n",
    "    rmse = np.sqrt(-scores['test_neg_mean_squared_error'])\n",
    "    r2 = scores['test_r2']\n",
    "    \n",
    "    return rmse,r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaaa577-86d6-4ade-9ef0-4a23e06aae6a",
   "metadata": {},
   "source": [
    "## Pre-process Data: Convert 2d sample to 1d sample (eg mean or percentile) or subset of pixels in 2d sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4896ad-79e9-44b2-b922-ec7d029dc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vector(channel: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize a vector to have zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    if channel.std() == 0:\n",
    "        return channel - channel.mean()\n",
    "    return (channel - channel.mean()) / channel.std()\n",
    "\n",
    "    #return (channel - channel.mean())\n",
    "\n",
    "def convert_3d_to_2d_concat(X) -> np.ndarray:\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = from_nested_to_3d_numpy(X)\n",
    "\n",
    "    X_2d = np.zeros((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            #X_2d[i, j * X.shape[2] : (j + 1) * X.shape[2]] = normalize_vector(X[i, j, :])\n",
    "            X_2d[i, j * X.shape[2] : (j + 1) * X.shape[2]] = X[i, j, :]\n",
    "\n",
    "    return X_2d\n",
    "\n",
    "def convert_3d_to_2d_percentile(X, perc = 25) ->  np.ndarray:\n",
    "    X_2d_perc = np.zeros((X.shape[0], X.shape[2]))\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        X_2d_perc[i, :] = np.percentile(X[i,:,:], q = perc, axis = 0)\n",
    "\n",
    "    return X_2d_perc\n",
    "\n",
    "def convert_3d_to_2d_mean(X) ->  np.ndarray:\n",
    "    X_2d_mean = np.zeros((X.shape[0], X.shape[2]))\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        X_2d_mean[i, :] = np.mean(X[i,:,:], axis = 0)\n",
    "\n",
    "    return X_2d_mean\n",
    "\n",
    "#From a 3d numpy array, compute percentiles and mean of channels, than put them together in a new 3d numpy array\n",
    "#This should reduce noise and outliers in the data, but allow the sample to preserve info about mean and variance.\n",
    "def convert_3d_to_3d_subset(X, percentiles = [25, 50, 75], add_mean = False) ->  np.ndarray:\n",
    "    \n",
    "    if add_mean == False:\n",
    "        X_3d_subset = np.zeros((X.shape[0], len(percentiles), X.shape[2]))\n",
    "        for i in range(X.shape[0]):\n",
    "            X_subset = np.percentile(X[i, :, :], q = percentiles, axis = 0)\n",
    "            X_3d_subset[i, :, :] = X_subset\n",
    "    else: \n",
    "        X_3d_subset = np.zeros((X.shape[0], len(percentiles) + 1, X.shape[2]))\n",
    "        for i in range(X.shape[0]):\n",
    "            X_mean = np.mean(X[i,:,:], axis = 0)\n",
    "            X_mean_2d = X_mean[np.newaxis, :]\n",
    "            #print(X_mean_2d.shape)\n",
    "            X_subset = np.percentile(X[i, :, :], q = percentiles, axis = 0)\n",
    "            X_3d_subset[i, :, :] = np.concatenate((X_subset, X_mean_2d))\n",
    "    \n",
    "    return X_3d_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7a428-6079-4693-a458-a2560144f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_columns', 10)\n",
    "\n",
    "#for sample in range(test_X.shape[0]):\n",
    "for sample in range(3):\n",
    "    print(\"sample:\", sample)\n",
    "    train_X_2d = convert_3d_to_2d_percentile(train_X, perc = 75)\n",
    "    df = pd.DataFrame(train_X_2d[sample])\n",
    "    #print(df)\n",
    "    #print(df.describe().T)\n",
    "    #print(train_y[sample])\n",
    "    #plotWaves(df = df, features = df.columns, target = [str(train_y[sample])], out_file=\"plot_train_sample\" + str(sample) + \"-target\" + str(train_y[sample]))\n",
    "    #plotWaves(df = df, features = df.columns, target = [\"NA\"], out_file=\"plot_test_sample\" + str(sample) + \"-target-na\")\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.plot(df, color='LightBlue')\n",
    "    plt.title('p75 sample')\n",
    "    target = [str(train_y[sample])]\n",
    "    plt.legend(target)\n",
    "    #plt.grid()\n",
    "    out_file=str(target)+\"p75_plot_sample.png\"\n",
    "    plt.savefig(out_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35d8eb-30ae-4a08-8690-27355d8a3ec8",
   "metadata": {},
   "source": [
    "## TABULAR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b28ea-4ecb-44f5-b9bc-c611e4bc7074",
   "metadata": {},
   "source": [
    "## Concatenate all rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82853085-e1f3-4b56-bde7-fe2d689c3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try tabular models by concatenating all rows into a single array\n",
    "models = [\n",
    "    #LinearRegression(),\n",
    "    #Ridge(),\n",
    "    RidgeCV(),\n",
    "    #RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "    #make_pipeline(VarianceThreshold(threshold=2.375211e+02),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "\n",
    "   #  make_pipeline(SelectKBest(f_regression, k=10000),\n",
    "   #         RidgeCV(),\n",
    "   #     ),\n",
    "    #make_pipeline(SelectKBest(mutual_info_regression, k=1000),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "    #make_pipeline(SelectKBest(f_regression, k=1000),\n",
    "    #        PolynomialFeatures(degree=2),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "    \n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "\n",
    "    #ExtraTreesRegressor(n_estimators=100),\n",
    "\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        SelectFromModel(RidgeCV()),\n",
    "    #        ExtraTreesRegressor(n_estimators=100),\n",
    "    #    ),\n",
    "    #Lasso(),\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        SelectFromModel(RidgeCV()),\n",
    "    #        Lasso(),\n",
    "    #    ),\n",
    "\n",
    "   # make_pipeline(\n",
    "   #         StandardScaler(),\n",
    "   #         SelectFromModel(RidgeCV()),\n",
    "   #         ElasticNet(),\n",
    "   #     ),\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        SelectFromModel(RidgeCV()),\n",
    "    #        PolynomialFeatures(degree=2),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "    \n",
    "   # make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "   #         StandardScaler(),\n",
    "   #         RidgeCV(),\n",
    "   #     ),\n",
    "   #KNeighborsRegressor(n_neighbors=1),\n",
    "    KNeighborsRegressor(n_neighbors=5),\n",
    "    #KNeighborsRegressor(n_neighbors=8),\n",
    "    KNeighborsRegressor(n_neighbors=10),\n",
    "    \n",
    "    make_pipeline(VarianceThreshold(threshold=100.949228),\n",
    "        KNeighborsRegressor(n_neighbors=5),\n",
    "        ),\n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            KNeighborsRegressor(n_neighbors=5),\n",
    "        ),\n",
    "    #MLPRegressor(), \n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #         MLPRegressor(), \n",
    "    #),\n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #            StandardScaler(),\n",
    "    #            MLPRegressor(), \n",
    "    #),\n",
    "    \n",
    "    #SVR(kernel='linear'), \n",
    "    \n",
    "    #SVR(kernel='rbf'),\n",
    "\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #          SVR(kernel='rbf'),\n",
    "    #),\n",
    "    #Lasso(),\n",
    "    #LassoCV(),\n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #        Lasso(),\n",
    "    #    ),\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        Lasso(),\n",
    "    #    ),\n",
    "    #ElasticNet(), \n",
    "    PLSRegression(n_components = 5),\n",
    "    PLSRegression(n_components = 10),\n",
    "    PLSRegression(n_components = 20),\n",
    "    #PLSRegression(n_components = 30),\n",
    "    \n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #            PLSRegression(n_components = 5),  \n",
    "    #    ),\n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            PLSRegression(n_components = 5),\n",
    "        ),\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        SelectFromModel(RidgeCV()),\n",
    "    #        RandomForestRegressor(),\n",
    "    #    ),\n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #            RandomForestRegressor(n_estimators=100),\n",
    "    #    ),\n",
    "    #mord.LogisticAT(alpha=1.),\n",
    "   \n",
    "]\n",
    "\n",
    "#clf2 = mord.LogisticAT(alpha=1.)\n",
    "#clf2.fit(X, y)\n",
    "#print('Mean Absolute Error of LogisticAT %s' %metrics.mean_absolute_error(clf2.predict(X), y))\n",
    "algos_df = pd.DataFrame({\"algo\":[], \"rmsecv\": []})\n",
    "\n",
    "if train_X.shape[0] > 1:\n",
    "    train_X_2d = convert_3d_to_2d_concat(train_X)\n",
    "    print(train_X_2d.shape)\n",
    "    #print(train_X_2d[0,:])\n",
    "    \n",
    "    for rgr in models:\n",
    "        print(\"\\n\", rgr)\n",
    "        rmse, r2 = cv_experiment(train_X_2d, train_y, rgr, cv=4)\n",
    "        print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        print(f'CV RMSE: {np.mean(rmse)} - CV R2: {np.mean(r2)}')\n",
    "        #algos_df = algos_df.append({\"percentile\":str(percentile),\"algo\":str(algo), \"rmsecv\": np.mean(rmse)}, ignore_index=True)\n",
    "        new_row = {\"algo\":str(rgr), \"rmsecv\": np.mean(rmse)}\n",
    "        algos_df = pd.concat([algos_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        rgr.fit(train_X_2d, train_y)\n",
    "        train_y_pred = rgr.predict(train_X_2d)\n",
    "        rmse = math.sqrt(mean_squared_error(train_y, train_y_pred))\n",
    "        r2 = r2_score(train_y, train_y_pred)\n",
    "        print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        \n",
    "        test_X_2d = convert_3d_to_2d_concat(test_X)\n",
    "        y_pred_test = rgr.predict(test_X_2d)\n",
    "        print(y_pred_test)\n",
    "\n",
    "print(algos_df.sort_values('rmsecv'))\n",
    "algos_df.sort_values('rmsecv').to_csv(\"tabular-concat\" + \"-rmsecv.csv\")\n",
    "\n",
    "    #plt.plot(y_pred,y_test)\n",
    "    #r = np.corrcoef(y_pred, y_test)[0,1]\n",
    "    #fig = px.scatter(training_df, x=y_pred, y=y_test,width=600, height=300)\n",
    "    #fig.update_layout(margin=dict(l=20, r=20, t=20, b=20), paper_bgcolor=\"LightSteelBlue\")\n",
    "    #fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a1311-2a40-4c13-ad9b-0d5622e737bf",
   "metadata": {},
   "source": [
    "## Flatten the data to a percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001260fe-2d9b-4005-8db7-d5c27f44b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try tabular models by concatenating all rows into a single array\n",
    "models = [\n",
    "    #LinearRegression(),\n",
    "    #Ridge(),\n",
    "    RidgeCV(),\n",
    "    #RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "    #make_pipeline(VarianceThreshold(threshold=2.375211e+02),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "\n",
    "   #  make_pipeline(SelectKBest(f_regression, k=10000),\n",
    "   #         RidgeCV(),\n",
    "   #     ),\n",
    "    #make_pipeline(SelectKBest(mutual_info_regression, k=1000),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "    #make_pipeline(SelectKBest(f_regression, k=1000),\n",
    "    #        PolynomialFeatures(degree=2),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "    \n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "\n",
    "    ExtraTreesRegressor(n_estimators=100),\n",
    "\n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            ExtraTreesRegressor(n_estimators=100),\n",
    "        ),\n",
    "    #Lasso(),\n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            Lasso(),\n",
    "        ),\n",
    "\n",
    "   # make_pipeline(\n",
    "   #         StandardScaler(),\n",
    "   #         SelectFromModel(RidgeCV()),\n",
    "   #         ElasticNet(),\n",
    "   #     ),\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        SelectFromModel(RidgeCV()),\n",
    "    #        PolynomialFeatures(degree=2),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "    \n",
    "   # make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "   #         StandardScaler(),\n",
    "   #         RidgeCV(),\n",
    "   #     ),\n",
    "   #KNeighborsRegressor(n_neighbors=1),\n",
    "    KNeighborsRegressor(n_neighbors=5),\n",
    "    #KNeighborsRegressor(n_neighbors=8),\n",
    "    KNeighborsRegressor(n_neighbors=10),\n",
    "    \n",
    "    make_pipeline(VarianceThreshold(threshold=100.949228),\n",
    "        KNeighborsRegressor(n_neighbors=5),\n",
    "        ),\n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            KNeighborsRegressor(n_neighbors=5),\n",
    "        ),\n",
    "    #MLPRegressor(), \n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #         MLPRegressor(), \n",
    "    #),\n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #            StandardScaler(),\n",
    "    #            MLPRegressor(), \n",
    "    #),\n",
    "    \n",
    "    #SVR(kernel='linear'), \n",
    "    \n",
    "    #SVR(kernel='rbf'),\n",
    "\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #          SVR(kernel='rbf'),\n",
    "    #),\n",
    "    #Lasso(),\n",
    "    #LassoCV(),\n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #        Lasso(),\n",
    "    #    ),\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        Lasso(),\n",
    "    #    ),\n",
    "    #ElasticNet(), \n",
    "    PLSRegression(n_components = 5),\n",
    "    PLSRegression(n_components = 10),\n",
    "    #PLSRegression(n_components = 20),\n",
    "    #PLSRegression(n_components = 30),\n",
    "    \n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #            PLSRegression(n_components = 5),  \n",
    "    #    ),\n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            PLSRegression(n_components = 5),\n",
    "        ),\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        SelectFromModel(RidgeCV()),\n",
    "    #        RandomForestRegressor(),\n",
    "    #    ),\n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #            RandomForestRegressor(n_estimators=100),\n",
    "    #    ),\n",
    "    #mord.LogisticAT(alpha=1.),\n",
    "   \n",
    "]\n",
    "\n",
    "#clf2 = mord.LogisticAT(alpha=1.)\n",
    "#clf2.fit(X, y)\n",
    "#print('Mean Absolute Error of LogisticAT %s' %metrics.mean_absolute_error(clf2.predict(X), y))\n",
    "algos_df = pd.DataFrame({\"percentile\":[], \"algo\":[], \"rmsecv\": []})\n",
    "\n",
    "percentiles = [25, 50, 75]\n",
    "for percentile in percentiles:\n",
    "    print(percentile)\n",
    "    if train_X.shape[0] > 1:\n",
    "        train_X_2d = convert_3d_to_2d_percentile(train_X, perc = percentile)\n",
    "    #train_X_2d = convert_3d_to_2d_concat(train_X)\n",
    "        #test_x = convert_3d_to_2d(test_x)\n",
    "        #print(train_X_2d.shape)\n",
    "        #print(train_X_2d[0,:])\n",
    "    \n",
    "    for m in models:\n",
    "        print(\"\\n\", m)\n",
    "        rmse, r2 = cv_experiment(train_X_2d, train_y, m, cv=4)\n",
    "        print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        print(f'CV RMSE: {np.mean(rmse)} - CV R2: {np.mean(r2)}')\n",
    "        #algos_df = algos_df.append({\"percentile\":str(percentile),\"algo\":str(algo), \"rmsecv\": np.mean(rmse)}, ignore_index=True)\n",
    "        new_row = {\"percentile\":str(percentile),\"algo\":str(m), \"rmsecv\": np.mean(rmse)}\n",
    "        algos_df = pd.concat([algos_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        \n",
    "        m.fit(train_X_2d, train_y)\n",
    "        train_y_pred = m.predict(train_X_2d)\n",
    "        rmse = math.sqrt(mean_squared_error(train_y, train_y_pred))\n",
    "        r2 = r2_score(train_y, train_y_pred)\n",
    "        print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        \n",
    "        test_X_2d = convert_3d_to_2d_percentile(test_X, perc = percentile)\n",
    "        y_pred_test = m.predict(test_X_2d)\n",
    "        print(y_pred_test)\n",
    "        \n",
    "print(algos_df.sort_values('rmsecv'))\n",
    "algos_df.sort_values('rmsecv').to_csv(\"tabular-percentiles\" + \"-rmsecv.csv\")\n",
    "\n",
    "    #plt.plot(y_pred,y_test)\n",
    "    #r = np.corrcoef(y_pred, y_test)[0,1]\n",
    "    #fig = px.scatter(training_df, x=y_pred, y=y_test,width=600, height=300)\n",
    "    #fig.update_layout(margin=dict(l=20, r=20, t=20, b=20), paper_bgcolor=\"LightSteelBlue\")\n",
    "    #fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435960e1-2d9e-469d-9669-ab495ae763cc",
   "metadata": {},
   "source": [
    "## Flatten to subset of percentiles, then concatenate into a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be69332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try tabular models by concatenating all rows into a single array\n",
    "random_state=42\n",
    "models = [\n",
    "    #LinearRegression(),\n",
    "    #Ridge(),\n",
    "    RidgeCV(),\n",
    "    #RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "    #make_pipeline(VarianceThreshold(threshold=2.375211e+02),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "\n",
    "   #  make_pipeline(SelectKBest(f_regression, k=10000),\n",
    "   #         RidgeCV(),\n",
    "   #     ),\n",
    "    #make_pipeline(SelectKBest(mutual_info_regression, k=1000),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "    #make_pipeline(SelectKBest(f_regression, k=1000),\n",
    "    #        PolynomialFeatures(degree=2),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "    \n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "    #Lasso(),\n",
    "   # make_pipeline(\n",
    "   #         StandardScaler(),\n",
    "   #         SelectFromModel(RidgeCV()),\n",
    "   #         Lasso(),\n",
    "   #     ),\n",
    "\n",
    "   # make_pipeline(\n",
    "   #         StandardScaler(),\n",
    "   #         SelectFromModel(RidgeCV()),\n",
    "   #         ElasticNet(),\n",
    "   #     ),\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        SelectFromModel(RidgeCV()),\n",
    "    #        PolynomialFeatures(degree=2),\n",
    "    #        RidgeCV(),\n",
    "    #    ),\n",
    "    \n",
    "   # make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "   #         StandardScaler(),\n",
    "   #         RidgeCV(),\n",
    "   #     ),\n",
    "   #KNeighborsRegressor(n_neighbors=1),\n",
    "   # KNeighborsRegressor(n_neighbors=5),\n",
    "    #KNeighborsRegressor(n_neighbors=8),\n",
    "    KNeighborsRegressor(n_neighbors=10),\n",
    "    \n",
    "    #make_pipeline(VarianceThreshold(threshold=100.949228),\n",
    "    #    KNeighborsRegressor(n_neighbors=5),\n",
    "    #    ),\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        SelectFromModel(RidgeCV()),\n",
    "    #        KNeighborsRegressor(n_neighbors=5),\n",
    "    #    ),\n",
    "    #MLPRegressor(), \n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #         MLPRegressor(), \n",
    "    #),\n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #            StandardScaler(),\n",
    "    #            MLPRegressor(), \n",
    "    #),\n",
    "    \n",
    "    #SVR(kernel='linear'), \n",
    "    \n",
    "    #SVR(kernel='rbf'),\n",
    "\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #          SVR(kernel='rbf'),\n",
    "    #),\n",
    "    #Lasso(),\n",
    "    #LassoCV(),\n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #        Lasso(),\n",
    "    #    ),\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        Lasso(),\n",
    "    #    ),\n",
    "    #ElasticNet(), \n",
    "    #PLSRegression(n_components = 5),\n",
    "    PLSRegression(n_components = 10),\n",
    "    #PLSRegression(n_components = 20),\n",
    "    #PLSRegression(n_components = 30),\n",
    "    \n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #            PLSRegression(n_components = 5),  \n",
    "    #    ),\n",
    "    make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            PLSRegression(n_components = 10),\n",
    "        ),\n",
    "    make_pipeline(\n",
    "            Rocket(random_state=random_state, normalise=True),\n",
    "            StandardScaler(),\n",
    "            RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        ),\n",
    "\n",
    "    make_pipeline(\n",
    "            Rocket(random_state=42, normalise=True),\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            #RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "            ExtraTreesRegressor(n_estimators=100),\n",
    "        ),\n",
    "\n",
    "    #make_pipeline(\n",
    "    #        StandardScaler(),\n",
    "    #        SelectFromModel(RidgeCV()),\n",
    "    #        RandomForestRegressor(),\n",
    "    #    ),\n",
    "    #make_pipeline(VarianceThreshold(threshold=1.639303e+02),\n",
    "    #            RandomForestRegressor(n_estimators=100),\n",
    "    #    ),\n",
    "    #mord.LogisticAT(alpha=1.),\n",
    "   \n",
    "]\n",
    "\n",
    "#clf2 = mord.LogisticAT(alpha=1.)\n",
    "#clf2.fit(X, y)\n",
    "#print('Mean Absolute Error of LogisticAT %s' %metrics.mean_absolute_error(clf2.predict(X), y))\n",
    "algos_df = pd.DataFrame({\"percentile\":[], \"algo\":[], \"rmsecv\": []})\n",
    "\n",
    "train_X_subset = convert_3d_to_3d_subset(train_X, percentiles = [25, 50, 75], add_mean = False)\n",
    "print(train_X_subset.shape)\n",
    "train_X_2d = convert_3d_to_2d_concat(train_X_subset)\n",
    "print(train_X_2d.shape)\n",
    "\n",
    "percentile=\"25-50-75\"\n",
    "for m in models:\n",
    "    print(\"\\n\", m)\n",
    "\n",
    "    rmse, r2 = cv_experiment(train_X_2d, train_y, m, cv=4)\n",
    "    print(f'RMSE: {rmse} - R2: {r2}')\n",
    "    print(f'CV RMSE: {np.mean(rmse)} - CV R2: {np.mean(r2)}')\n",
    "        #algos_df = algos_df.append({\"percentile\":str(percentile),\"algo\":str(algo), \"rmsecv\": np.mean(rmse)}, ignore_index=True)\n",
    "    new_row = {\"percentile\":str(percentile),\"algo\":str(m), \"rmsecv\": np.mean(rmse)}\n",
    "    algos_df = pd.concat([algos_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        \n",
    "print(algos_df.sort_values('rmsecv'))\n",
    "algos_df.sort_values('rmsecv').to_csv(\"tabular-percentiles\" + \"-rmsecv.csv\")\n",
    "\n",
    "    #plt.plot(y_pred,y_test)\n",
    "    #r = np.corrcoef(y_pred, y_test)[0,1]\n",
    "    #fig = px.scatter(training_df, x=y_pred, y=y_test,width=600, height=300)\n",
    "    #fig.update_layout(margin=dict(l=20, r=20, t=20, b=20), paper_bgcolor=\"LightSteelBlue\")\n",
    "    #fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5aa579-b4d1-44c1-8302-6e67047239ae",
   "metadata": {},
   "source": [
    "## TIME SERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5cbc0e-02df-4171-af52-7b205852d096",
   "metadata": {},
   "source": [
    "## UNIVARIATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb8b59-9ca1-42bc-bef2-7c8ff0a811f3",
   "metadata": {},
   "source": [
    "## Flatten the 2d time series to a single percentile 1d time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f8bbf-77ba-4f64-98c4-01561ba909eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "models = [\n",
    "        make_pipeline(\n",
    "            Rocket(random_state=random_state, normalise=True),\n",
    "            RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            Rocket(random_state=random_state, normalise=True),\n",
    "            StandardScaler(),\n",
    "            RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            Rocket(random_state=random_state, normalise=True),\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        ),\n",
    "\n",
    "            make_pipeline(\n",
    "            Rocket(random_state=random_state, normalise=True),\n",
    "            StandardScaler(),\n",
    "            #RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "            ExtraTreesRegressor(n_estimators=100),\n",
    "        ),\n",
    "\n",
    "            make_pipeline(\n",
    "            Rocket(random_state=random_state, normalise=True),\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            #RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "            ExtraTreesRegressor(n_estimators=100),\n",
    "        ),\n",
    "  \n",
    "        RidgeCV(),\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "        #KNeighborsRegressor(n_neighbors=10),\n",
    "        #make_pipeline(\n",
    "        #    MiniRocket(random_state=random_state),\n",
    "        #    StandardScaler(),\n",
    "        #    RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        #),\n",
    "       # make_pipeline(\n",
    "       #     MultiRocket(random_state=random_state),\n",
    "       #     StandardScaler(),\n",
    "       #     RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "       # ),\n",
    "        #RDSTRegressor(),\n",
    "        #RDSTRegressor(),\n",
    "        #RISTRegressor(n_jobs=-1, random_state=random_state),\n",
    "    \n",
    "        #make_pipeline(\n",
    "          #  StandardScaler(),\n",
    "        #   TimeSeriesForestRegressor()\n",
    "        #),\n",
    "          # DrCIFRegressor()\n",
    "\n",
    "]\n",
    "\n",
    "algos_df = pd.DataFrame({\"percentile\":[], \"algo\":[], \"rmsecv\": []})\n",
    "\n",
    "percentiles = [25, 50, 75]\n",
    "for percentile in percentiles:\n",
    "    print(percentile)\n",
    "    if train_X.shape[0] > 1:\n",
    "        train_X_2d = convert_3d_to_2d_percentile(train_X, perc = percentile)\n",
    "        #train_X_2d = convert_3d_to_2d_mean(train_X)\n",
    "        #train_X_2d = convert_3d_to_2d_percentile(X_selected, perc = percentile)\n",
    "        #train_X_2d = convert_3d_to_2d_concat(train_X)\n",
    "        #test_x = convert_3d_to_2d(test_x)\n",
    "        #print(train_X_2d.shape)\n",
    "        #print(train_X_2d[0,:])\n",
    "    \n",
    "    for m in models:\n",
    "        print(\"\\n\", m)\n",
    "        starttime = timeit.default_timer()\n",
    "        rmse, r2 = cv_experiment(train_X_2d, train_y, m, cv=4)\n",
    "        print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        print(f'CV RMSE: {np.mean(rmse)} - CV R2: {np.mean(r2)}')\n",
    "        #algos_df = algos_df.append({\"percentile\":str(percentile),\"algo\":str(algo), \"rmsecv\": np.mean(rmse)}, ignore_index=True)\n",
    "        new_row = {\"percentile\":str(percentile),\"algo\":str(m), \"rmsecv\": np.mean(rmse)}\n",
    "        algos_df = pd.concat([algos_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        print(\"Time to train + test (sec):\", timeit.default_timer() - starttime)\n",
    "\n",
    "        m.fit(train_X_2d, train_y)\n",
    "        train_y_pred = m.predict(train_X_2d)\n",
    "        rmse = math.sqrt(mean_squared_error(train_y, train_y_pred))\n",
    "        r2 = r2_score(train_y, train_y_pred)\n",
    "        print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        \n",
    "        test_X_2d = convert_3d_to_2d_percentile(test_X, perc = percentile)\n",
    "        y_pred_test = m.predict(test_X_2d)\n",
    "        print(y_pred_test)        \n",
    "        \n",
    "print(algos_df.sort_values('rmsecv'))\n",
    "#algos_df.sort_values('rmsecv').to_csv(\"UTS-percentiles\" + \"-rmsecv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86485bb3-ff4f-4e18-898f-aa85253e65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "rgr = make_pipeline(\n",
    "            Rocket(random_state=random_state, normalise=True),\n",
    "            StandardScaler(),\n",
    "            RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        )\n",
    "\n",
    "#rgr =  KNeighborsRegressor(n_neighbors=10)\n",
    "\n",
    "percentiles = [\"mean\"]\n",
    "for percentile in percentiles:\n",
    "    print(percentile)\n",
    "\n",
    "    \n",
    "    if train_X.shape[0] > 1:\n",
    "        #train_X_2d = convert_3d_to_2d_percentile(train_X, perc = percentile)\n",
    "        train_X_2d = convert_3d_to_2d_mean(train_X)\n",
    "        #train_X_2d = convert_3d_to_2d_percentile(X_selected, perc = percentile)\n",
    "        #train_X_2d = convert_3d_to_2d_concat(train_X)\n",
    "        #test_x = convert_3d_to_2d(test_x)\n",
    "        #print(train_X_2d.shape)\n",
    "        #print(train_X_2d[0,:])\n",
    "\n",
    "        rgr.fit(train_X_2d, train_y)\n",
    "        train_y_pred = rgr.predict(train_X_2d)\n",
    "        rmse = math.sqrt(mean_squared_error(train_y, train_y_pred))\n",
    "        r2 = r2_score(train_y, train_y_pred)\n",
    "        print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        \n",
    "        #test_X_2d = convert_3d_to_2d_percentile(test_X, perc = percentile)\n",
    "        test_X_2d = convert_3d_to_2d_mean(test_X)\n",
    "        y_pred_test = rgr.predict(test_X_2d)\n",
    "        print(y_pred_test)       \n",
    "'''        \n",
    "72    8\n",
    "62    8\n",
    "60    8\n",
    "48    8\n",
    "24    8\n",
    "58    8\n",
    "29    8\n",
    "19    8\n",
    "'''\n",
    "y_test_human = [72, 62, 24, 62, 29, 58, 48, 19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33bdf41-7de3-464a-acc6-c878e1d6e05b",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cb802-6ef8-4557-b283-c9f4fbf3722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [75]\n",
    "\n",
    "for i in range(0, 1):\n",
    "    \n",
    "    rgr = make_pipeline(\n",
    "            Rocket(normalise=True, num_kernels=10000, n_jobs=-1, random_state=142),\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            ExtraTreesRegressor(n_estimators=100, random_state=142),\n",
    "            #RandomForestRegressor(n_estimators=100, random_state=142),\n",
    "                \n",
    "        )\n",
    "\n",
    "    for percentile in percentiles:\n",
    "        starttime = timeit.default_timer()\n",
    "        print(percentile)\n",
    "        if train_X.shape[0] > 1:\n",
    "            train_X_2d = convert_3d_to_2d_percentile(train_X, perc = percentile)\n",
    "            print(\"\\n\", rgr)\n",
    "            rmse, r2 = cv_experiment(train_X_2d, train_y, rgr, cv=4)\n",
    "            print(f'RMSE: {rmse} - R2: {r2}')\n",
    "            print(f'CV RMSE: {np.mean(rmse)} - CV R2: {np.mean(r2)}')\n",
    "            print(\"Time to train + test (sec):\", timeit.default_timer() - starttime)\n",
    "\n",
    "            starttime = timeit.default_timer()\n",
    "            rgr.fit(train_X_2d, train_y)\n",
    "            train_y_pred = rgr.predict(train_X_2d)\n",
    "            rmse = math.sqrt(mean_squared_error(train_y, train_y_pred))\n",
    "            r2 = r2_score(train_y, train_y_pred)\n",
    "            print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        \n",
    "            test_X_2d = convert_3d_to_2d_percentile(test_X, perc = percentile)\n",
    "            #test_X_2d = pd.DataFrame(savgol_filter(test_X_2d, window_length = 5, polyorder = 2, deriv=0, axis=1))\n",
    "            y_pred_test = rgr.predict(test_X_2d)\n",
    "            print(y_pred_test)        \n",
    "            print(\"Time to train + test (sec):\", timeit.default_timer() - starttime)\n",
    "#y_test_human = [72, 60, 24, 62, 29, 58, 48, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac56bf-235d-4456-8a0b-02c4582c1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X_2d.shape)\n",
    "# apply ROCKET transform to create a vector with 20k features\n",
    "rgr1 = Rocket(normalise=True, num_kernels=10000, n_jobs=-1, random_state=142)\n",
    "X1 = rgr1.fit_transform(train_X_2d)\n",
    "print(rgr1)\n",
    "print(rgr1.get_params())\n",
    "print(X1.shape)\n",
    "#scale the 20k features\n",
    "rgr2 = StandardScaler()\n",
    "X2 = rgr2.fit_transform(X1)\n",
    "print(rgr2)\n",
    "print(X2.shape)\n",
    "#apply feature selection to reduce the 20k features to a subset\n",
    "rgr3 = SelectFromModel(RidgeCV())\n",
    "X3 = rgr3.fit_transform(X2, train_y)\n",
    "print(rgr3)\n",
    "print(X3.shape)\n",
    "#look at how many features are selected using SelectFromModel()\n",
    "print(\"threshold: \", rgr3.threshold_)\n",
    "print(\"number of selected features: \", rgr3.get_support().sum())\n",
    "#Train an ExtraTrees ensemble with the feature subset\n",
    "rgr4 =  ExtraTreesRegressor(n_estimators=100, random_state=142)\n",
    "rgr4.fit(X3, train_y)\n",
    "print(rgr4)\n",
    "# and the feature importance from ExtraTrees ensemble\n",
    "#feature_idx = [i for i in range(1,train_X_2d.shape[1]+1)]\n",
    "#print(feature_idx)\n",
    "#feature_importance = pd.DataFrame({'feature': feature_idx, 'importance':rgr4.feature_importances_})\n",
    "feature_importance = pd.DataFrame({'importance':rgr4.feature_importances_})\n",
    "print(feature_importance.sort_values('importance', ascending=False))\n",
    "#print(feature_importance.sort_values('importance', ascending=False)[:10])\n",
    "print((feature_importance['importance'] != 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfce13-e5aa-4e0d-ad61-a9b05e5677b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "rgr_list = []\n",
    "for i in range(0,5):\n",
    "    rgr_single = make_pipeline(\n",
    "            make_pipeline(\n",
    "            Rocket(normalise=True, num_kernels=10000, n_jobs=-1),\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            #RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "            ExtraTreesRegressor(n_estimators=100),\n",
    "        ))\n",
    "    rgr_list.append((\"rgr\"+str(i),rgr_single))\n",
    "\n",
    "#print(rgr_list)\n",
    "\n",
    "rgr =  VotingRegressor(estimators=rgr_list)\n",
    "\n",
    "for percentile in [75]:\n",
    "    print(percentile)\n",
    "    if train_X.shape[0] > 1:\n",
    "        train_X_2d = convert_3d_to_2d_percentile(train_X, perc = percentile)\n",
    "        #train_X_2d = pd.DataFrame(savgol_filter(train_X_2d, window_length = 5, polyorder = 2, deriv=0, axis=1))\n",
    "        \n",
    "        print(\"\\n\", rgr)\n",
    "        rmse, r2 = cv_experiment(train_X_2d, train_y, rgr, cv=4)\n",
    "        print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        print(f'CV RMSE: {np.mean(rmse)} - CV R2: {np.mean(r2)}')\n",
    "        \n",
    "        rgr.fit(train_X_2d, train_y)\n",
    "        train_y_pred = rgr.predict(train_X_2d)\n",
    "        rmse = math.sqrt(mean_squared_error(train_y, train_y_pred))\n",
    "        r2 = r2_score(train_y, train_y_pred)\n",
    "        print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        \n",
    "        test_X_2d = convert_3d_to_2d_percentile(test_X, perc = percentile)\n",
    "        #test_X_2d = pd.DataFrame(savgol_filter(test_X_2d, window_length = 5, polyorder = 2, deriv=0, axis=1))\n",
    "        y_pred_test = rgr.predict(test_X_2d)\n",
    "        print(y_pred_test)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968b438-09c5-4c9f-81bd-196c6f7dc76b",
   "metadata": {},
   "source": [
    "## MULTIVARIATE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc7df1-92d5-48e6-84ca-1bedf8e0854c",
   "metadata": {},
   "source": [
    "## All channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798644c5-ac14-4ae8-ace9-4b5caf5c2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "models = [\n",
    "        make_pipeline(\n",
    "            Rocket(random_state=random_state),\n",
    "            StandardScaler(),\n",
    "            RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            MiniRocketMultivariate(random_state=random_state),\n",
    "            StandardScaler(),\n",
    "            RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            MultiRocketMultivariate(random_state=random_state),\n",
    "            StandardScaler(),\n",
    "            RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        ),\n",
    "]\n",
    "\n",
    "for m in models:\n",
    "    print(m)\n",
    "    rmse, r2, y_pred, y_test = single_split_experiment(train_X, train_y, m, stratify = train_y)\n",
    "    print(f'RMSE: {rmse} - R2: {r2}')\n",
    "\n",
    "    actual_vs_predicted = pd.concat([y_test, pd.DataFrame(y_pred, columns=['Predicted'], index=y_test.index)], axis=1)\n",
    "    print(actual_vs_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaac6ea-74f2-4a63-bb1b-c1bd6f64ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "models = [\n",
    "    \n",
    "    make_pipeline(\n",
    "            Rocket(random_state=random_state),\n",
    "            StandardScaler(),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "    \n",
    "        make_pipeline(\n",
    "            Rocket(random_state=random_state),\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "\n",
    "         make_pipeline(\n",
    "            Rocket(random_state=random_state),\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            #RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "            ExtraTreesRegressor(n_estimators=100),\n",
    "        ),\n",
    "        '''\n",
    "        make_pipeline(\n",
    "            MiniRocketMultivariate(random_state=random_state),\n",
    "            #StandardScaler(),\n",
    "            RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            MultiRocketMultivariate(random_state=random_state),\n",
    "            #StandardScaler(),\n",
    "            RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        ),\n",
    "        '''\n",
    "]\n",
    "\n",
    "for m in models:\n",
    "    print(m)\n",
    "    starttime = timeit.default_timer()\n",
    "    rmse, r2 = cv_experiment(train_X, train_y, m, cv=4)\n",
    "    print(f'RMSE: {rmse} - R2: {r2}')\n",
    "    print(f'CV RMSE: {np.mean(rmse)} - CV R2: {np.mean(r2)}')\n",
    "\n",
    "\n",
    "    m.fit(train_X, train_y)\n",
    "    train_y_pred = m.predict(train_X)\n",
    "    rmse = math.sqrt(mean_squared_error(train_y, train_y_pred))\n",
    "    r2 = r2_score(train_y, train_y_pred)\n",
    "    print(f'RMSE: {rmse} - R2: {r2}')\n",
    "        \n",
    "    y_pred_test = m.predict(test_X)\n",
    "    print(y_pred_test)     \n",
    "    print(\"Time to train + test (sec):\", timeit.default_timer() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c988c3-03f6-4903-b2eb-2b185ece3669",
   "metadata": {},
   "source": [
    "## Using a subset of channels: select [p25, p50, p75] percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfed159-fa64-43fc-85dd-c8dba97f23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the MTS to a subset including a few percentiles to remove the impact of outliers\n",
    "random_state = 42\n",
    "models = [\n",
    "      make_pipeline(\n",
    "            Rocket(random_state=random_state, normalise=True),\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(estimator=RidgeCV()),\n",
    "            RidgeCV(),\n",
    "        ),\n",
    "    \n",
    "        # make_pipeline(\n",
    "        #    HydraTransformer(),\n",
    "        #    StandardScaler(),\n",
    "        #    SelectFromModel(estimator=RidgeCV()), \n",
    "        #    RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        #),\n",
    "    make_pipeline(\n",
    "            Rocket(random_state=random_state),\n",
    "            StandardScaler(),\n",
    "            SelectFromModel(RidgeCV()),\n",
    "            #RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "            ExtraTreesRegressor(),\n",
    "        ),\n",
    "#        make_pipeline(\n",
    "#            StandardScaler(),\n",
    "#            SelectFromModel(estimator=RidgeCV()),\n",
    "#           DrCIFRegressor()\n",
    "#        ),\n",
    " #       RISTRegressor()\n",
    "       # make_pipeline(\n",
    "       #     MiniRocketMultivariate(random_state=random_state),\n",
    "            #StandardScaler(),\n",
    "       #     RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "       # ),\n",
    "       # make_pipeline(\n",
    "       #     MultiRocketMultivariate(random_state=random_state),\n",
    "            #StandardScaler(),\n",
    "       #     RidgeCV(alphas=np.logspace(-3, 3, 10)),\n",
    "        #),\n",
    "]\n",
    "percentiles = [25, 50, 75]\n",
    "train_X_subset = convert_3d_to_3d_subset(train_X, percentiles = [25, 50, 75], add_mean = False)\n",
    "test_X_subset = convert_3d_to_3d_subset(test_X, percentiles = [25, 50, 75], add_mean = False)\n",
    "print(train_X_subset.shape)\n",
    "print(percentiles)\n",
    "algos_df = pd.DataFrame({\"percentile\":[], \"algo\":[], \"rmsecv\": []})\n",
    "\n",
    "for m in models:\n",
    "    print(m)\n",
    "    starttime = timeit.default_timer()\n",
    "    rmse, r2 = cv_experiment(train_X_subset, train_y, m, cv=4)\n",
    "    print(f'RMSE: {rmse} - R2: {r2}')\n",
    "    print(f'CV RMSE: {np.mean(rmse)} - CV R2: {np.mean(r2)}')\n",
    "    new_row = {\"percentile\":str(percentiles),\"algo\":str(m), \"rmsecv\": np.mean(rmse)}\n",
    "    algos_df = pd.concat([algos_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    print(\"Time to train + test (sec):\", timeit.default_timer() - starttime)\n",
    "\n",
    "    m.fit(train_X_subset, train_y)\n",
    "    train_y_pred = m.predict(train_X_subset)\n",
    "    rmse = math.sqrt(mean_squared_error(train_y, train_y_pred))\n",
    "    r2 = r2_score(train_y, train_y_pred)\n",
    "    print(f'RMSE: {rmse} - R2: {r2}')\n",
    "\n",
    "    y_pred_test = m.predict(test_X_subset)\n",
    "    print(y_pred_test)     \n",
    "        \n",
    "print(algos_df.sort_values('rmsecv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505ad0e-1353-4bf6-bdc3-3747e89066fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa00b0-7f6c-49af-92a8-ef6b716523e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
